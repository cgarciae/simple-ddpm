{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38084995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 500\n",
    "    total_samples: int = 5_000_000\n",
    "    lr: float = 1e-3\n",
    "    num_steps: int = 100\n",
    "    schedule_exponent: float = 2.0\n",
    "\n",
    "    @property\n",
    "    def steps_per_epoch(self) -> int:\n",
    "        return self.total_samples // (self.epochs * self.batch_size)\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not get_ipython():\n",
    "    import sys\n",
    "\n",
    "    from absl import flags\n",
    "    from ml_collections import config_flags\n",
    "\n",
    "    config_flag = config_flags.DEFINE_config_dataclass(\"config\", config)\n",
    "    flags.FLAGS(sys.argv)\n",
    "    config = config_flag.value\n",
    "\n",
    "\n",
    "def show_interactive():\n",
    "    if not get_ipython():\n",
    "        plt.ion()\n",
    "        plt.pause(1)\n",
    "        plt.ioff()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdcb884",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets.load import load_dataset\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    ds = load_dataset(\"mnist\", split=\"train\")\n",
    "    X = np.stack(ds[\"image\"])[..., None]\n",
    "    return X / 127.5 - 1.0\n",
    "\n",
    "\n",
    "def render_image(x, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    x = (x[..., 0] + 1) * 127.5\n",
    "    x = np.clip(x, 0, 255).astype(np.uint8)\n",
    "    ax.imshow(255 - x, cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "X = get_data()\n",
    "\n",
    "x = X[np.random.choice(len(X), 8)]\n",
    "_, axs_diffusion = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i, ax in enumerate(axs_diffusion.flatten()):\n",
    "    render_image(x[i], ax=ax)\n",
    "\n",
    "show_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431ed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flax.struct import PyTreeNode\n",
    "\n",
    "\n",
    "def expand_to(a, b):\n",
    "    new_shape = a.shape + (1,) * (b.ndim - a.ndim)\n",
    "    return a.reshape(new_shape)\n",
    "\n",
    "\n",
    "class GaussianDiffusion(PyTreeNode):\n",
    "    betas: jnp.ndarray\n",
    "    alphas: jnp.ndarray\n",
    "    alpha_bars: jnp.ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, betas: jnp.ndarray) -> \"GaussianDiffusion\":\n",
    "        return cls(\n",
    "            betas=betas,\n",
    "            alphas=1.0 - betas,\n",
    "            alpha_bars=jnp.cumprod(1.0 - betas),\n",
    "        )\n",
    "\n",
    "\n",
    "def forward_diffusion(process, key, x0, t):\n",
    "    alpha_bars = expand_to(process.alpha_bars[t], x0)\n",
    "    noise = jax.random.normal(key, x0.shape)\n",
    "    xt = jnp.sqrt(alpha_bars) * x0 + jnp.sqrt(1.0 - alpha_bars) * noise\n",
    "    return xt, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee32da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_schedule(beta_start, beta_end, timesteps, exponent=2.0, **kwargs):\n",
    "    betas = jnp.linspace(0, 1, timesteps) ** exponent\n",
    "    return betas * (beta_end - beta_start) + beta_start\n",
    "\n",
    "\n",
    "def sigmoid_schedule(beta_start, beta_end, timesteps, **kwargs):\n",
    "    betas = jax.nn.sigmoid(jnp.linspace(-6, 6, timesteps))\n",
    "    return betas * (beta_end - beta_start) + beta_start\n",
    "\n",
    "\n",
    "def cosine_schedule(beta_start, beta_end, timesteps, s=0.008, **kwargs):\n",
    "    x = jnp.linspace(0, timesteps, timesteps + 1)\n",
    "    ft = jnp.cos(((x / timesteps) + s) / (1 + s) * jnp.pi * 0.5) ** 2\n",
    "    alphas_cumprod = ft / ft[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    betas = jnp.clip(betas, 0.0001, 0.9999)\n",
    "    betas = (betas - betas.min()) / (betas.max() - betas.min())\n",
    "    return betas * (beta_end - beta_start) + beta_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de30ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = cosine_schedule(\n",
    "    0.0001, 0.5, config.num_steps, exponent=config.schedule_exponent\n",
    ")\n",
    "process = GaussianDiffusion.create(betas)\n",
    "\n",
    "x = X[:1]\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i, ti in enumerate(jnp.linspace(0, config.num_steps, 5).astype(int)):\n",
    "    t = jnp.full((x.shape[0],), ti)\n",
    "    xt, noise = forward_diffusion(process, jax.random.PRNGKey(ti), x, t)\n",
    "    ax = plt.subplot(2, 5, i + 1)\n",
    "    render_image(xt[i], ax=ax)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "linear = polynomial_schedule(betas.min(), betas.max(), config.num_steps, exponent=1.0)\n",
    "plt.plot(linear, label=\"linear\", color=\"black\", linestyle=\"dotted\")\n",
    "plt.plot(betas)\n",
    "for s in [\"top\", \"bottom\", \"left\", \"right\"]:\n",
    "    plt.gca().spines[s].set_visible(False)\n",
    "\n",
    "show_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d8939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "from einop import einop\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    dim: int\n",
    "\n",
    "    def __call__(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        mul = jnp.log(10000) / (half_dim - 1)\n",
    "        emb = jnp.exp(-mul * jnp.arange(half_dim))\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = jnp.concatenate([jnp.sin(emb), jnp.cos(emb)], axis=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class TimeConditioned(nn.Module):\n",
    "    emb_dim: int\n",
    "    module: nn.Module\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t):\n",
    "        t_embeddings = PositionalEmbedding(self.emb_dim)(t)\n",
    "        axis = {f\"a{i}\": dim for i, dim in enumerate(x.shape[1:-1])}\n",
    "        t_embeddings = einop(t_embeddings, f\"b c -> b {' '.join(axis)} c\", **axis)\n",
    "        x = jnp.concatenate([x, t_embeddings], axis=-1)\n",
    "        x = self.module(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TimeConditionedMLP(nn.Module):\n",
    "    units: int = 128\n",
    "    emb_dim: int = 32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t):\n",
    "        input_shape = x.shape\n",
    "        inputs_units = np.prod(input_shape[1:])\n",
    "        x = x.reshape(-1, inputs_units)\n",
    "        dense = lambda units: TimeConditioned(self.emb_dim, nn.Dense(units))\n",
    "        x = nn.relu(dense(self.units)(x, t))\n",
    "        x = nn.relu(dense(self.units)(x, t)) + x\n",
    "        x = nn.relu(dense(self.units)(x, t)) + x\n",
    "        x = dense(inputs_units)(x, t)\n",
    "        x = x.reshape(*input_shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    units: int = 128\n",
    "    emb_dim: int = 32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t):\n",
    "        # Downsample\n",
    "        x = skip_0 = TimeConditioned(self.emb_dim, nn.Conv(32, (5, 5), padding=\"SAME\"))(\n",
    "            x, t\n",
    "        )\n",
    "        x = nn.GroupNorm(8)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = TimeConditioned(\n",
    "            self.emb_dim, nn.Conv(64, (5, 5), strides=(2, 2), padding=\"SAME\")\n",
    "        )(x, t)\n",
    "        x = nn.GroupNorm(16)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = skip_1 = TimeConditioned(self.emb_dim, nn.Conv(64, (3, 3), padding=\"SAME\"))(\n",
    "            x, t\n",
    "        )\n",
    "        x = nn.GroupNorm(16)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = TimeConditioned(\n",
    "            self.emb_dim, nn.Conv(128, (3, 3), strides=(2, 2), padding=\"SAME\")\n",
    "        )(x, t)\n",
    "        x = nn.GroupNorm(16)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = TimeConditioned(self.emb_dim, nn.Conv(128, (3, 3), padding=\"SAME\"))(x, t)\n",
    "        x = nn.GroupNorm(16)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        # Upsample\n",
    "        x = TimeConditioned(\n",
    "            self.emb_dim, nn.ConvTranspose(128, (3, 3), strides=(2, 2))\n",
    "        )(x, t)\n",
    "        x = jnp.concatenate([x, skip_1], axis=-1)\n",
    "        x = nn.GroupNorm(16)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = TimeConditioned(self.emb_dim, nn.Conv(128, (3, 3), padding=\"SAME\"))(x, t)\n",
    "        x = nn.GroupNorm(16)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = TimeConditioned(\n",
    "            self.emb_dim, nn.ConvTranspose(128, (3, 3), strides=(2, 2))\n",
    "        )(x, t)\n",
    "        x = jnp.concatenate([x, skip_0], axis=-1)\n",
    "        x = nn.GroupNorm(16)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = TimeConditioned(self.emb_dim, nn.Conv(1, (5, 5), padding=\"SAME\"))(x, t)\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    units: int = 128\n",
    "    emb_dim: int = 32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t):\n",
    "        input_units = x.shape[-1]\n",
    "        conv = lambda kernel_size, stride=1, **kwargs: TimeConditioned(\n",
    "            self.emb_dim,\n",
    "            nn.Conv(\n",
    "                self.units,\n",
    "                kernel_size=(kernel_size, kernel_size),\n",
    "                strides=(stride, stride),\n",
    "                padding=\"SAME\",\n",
    "                **kwargs,\n",
    "            ),\n",
    "        )\n",
    "        conv_trans = lambda kernel_size, stride=1, **kwargs: TimeConditioned(\n",
    "            self.emb_dim,\n",
    "            nn.ConvTranspose(\n",
    "                self.units,\n",
    "                kernel_size=(kernel_size, kernel_size),\n",
    "                strides=(stride, stride),\n",
    "                padding=\"SAME\",\n",
    "                **kwargs,\n",
    "            ),\n",
    "        )\n",
    "        norm = lambda n: nn.GroupNorm(n)\n",
    "\n",
    "        # Downsample\n",
    "        x = conv(5)(x, t)\n",
    "        x = norm(8)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        x = conv(5, stride=2)(x, t)\n",
    "        x = norm(8)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        x = conv(3)(x, t)\n",
    "        x = norm(8)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        # Upsample\n",
    "        x = conv_trans(5, stride=2)(x, t)\n",
    "        x = norm(8)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        x = conv(5)(x, t)\n",
    "        x = norm(8)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        x = nn.Dense(input_units)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP1D(nn.Module):\n",
    "    out_dim: int\n",
    "    units: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.units)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.out_dim)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MixerBlock1D(nn.Module):\n",
    "    mix_patch_size: int\n",
    "    mix_hidden_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        _, num_patches, hidden_size = x.shape\n",
    "        patch_mixer = MLP1D(num_patches, self.mix_patch_size)\n",
    "        hidden_mixer = MLP1D(hidden_size, self.mix_hidden_size)\n",
    "        norm1 = nn.LayerNorm()\n",
    "        norm2 = nn.LayerNorm()\n",
    "\n",
    "        x = einop(x, \"... p c -> ... c p\")\n",
    "        x = x + patch_mixer(norm1(x))\n",
    "        x = einop(x, \"... c p -> ... p c\")\n",
    "        x = x + hidden_mixer(norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mixer1D(nn.Module):\n",
    "    patch_size: int\n",
    "    hidden_size: int\n",
    "    mix_patch_size: int\n",
    "    mix_hidden_size: int\n",
    "    num_blocks: int\n",
    "    num_steps: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t):\n",
    "        input_size = x.shape[-1]\n",
    "        batch_size = x.shape[0]\n",
    "        height, width = x.shape[-3], x.shape[-2]\n",
    "        # ----------------\n",
    "        # setup\n",
    "        # ----------------\n",
    "\n",
    "        conv_in = nn.Conv(\n",
    "            self.hidden_size,\n",
    "            kernel_size=(self.patch_size, self.patch_size),\n",
    "            strides=(self.patch_size, self.patch_size),\n",
    "        )\n",
    "        conv_out = nn.ConvTranspose(\n",
    "            input_size,\n",
    "            kernel_size=(self.patch_size, self.patch_size),\n",
    "            strides=(self.patch_size, self.patch_size),\n",
    "        )\n",
    "        blocks = [\n",
    "            MixerBlock1D(self.mix_patch_size, self.mix_hidden_size)\n",
    "            for _ in range(self.num_blocks)\n",
    "        ]\n",
    "        norm = nn.LayerNorm()\n",
    "\n",
    "        ################\n",
    "        t = t / self.num_steps\n",
    "        t = einop(t, \"b -> b h w 1\", b=batch_size, h=height, w=width)\n",
    "        x = jnp.concatenate([x, t], axis=-1)\n",
    "        x = conv_in(x)\n",
    "        _, patch_height, patch_width, _ = x.shape\n",
    "        x = einop(x, \"b h w c -> b (h w) c\")\n",
    "        for block in blocks:\n",
    "            x = block(x)\n",
    "        x = norm(x)\n",
    "        x = einop(x, \"b (h w) c -> b h w c\", h=patch_height, w=patch_width)\n",
    "        return conv_out(x)\n",
    "\n",
    "\n",
    "class MLP2D(nn.Module):\n",
    "    units: int\n",
    "    out_dim: Optional[int] = None\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t):\n",
    "        out_dim = self.out_dim or x.shape[-1]\n",
    "        x = TimeConditioned(32, nn.Dense(self.units))(x, t)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(out_dim)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MixerBlock2D(nn.Module):\n",
    "    units: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t):\n",
    "        mlp = lambda x, t: MLP2D(self.units)(x, t)\n",
    "        norm = lambda x: nn.LayerNorm()(x)\n",
    "\n",
    "        x = x + mlp(norm(x), t)  # mix channels\n",
    "        x = einop(x, \"... h w c -> ... c h w\")\n",
    "        x = x + mlp(norm(x), t)  # mix width\n",
    "        x = einop(x, \"... c h w -> ... w c h\")\n",
    "        x = x + mlp(norm(x), t)  # mix height\n",
    "        x = einop(x, \"... w c h -> ... h w c\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mixer2D(nn.Module):\n",
    "    units: int\n",
    "    hidden_units: int\n",
    "    patch_size: Tuple[int, int]\n",
    "    num_blocks: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, t):\n",
    "        input_size = x.shape[-1]\n",
    "\n",
    "        conv_in = nn.Conv(\n",
    "            self.units,\n",
    "            kernel_size=self.patch_size,\n",
    "            strides=self.patch_size,\n",
    "        )\n",
    "        conv_out = nn.ConvTranspose(\n",
    "            input_size,\n",
    "            kernel_size=self.patch_size,\n",
    "            strides=self.patch_size,\n",
    "        )\n",
    "        blocks = [MixerBlock2D(self.hidden_units) for _ in range(self.num_blocks)]\n",
    "        norm = nn.LayerNorm()\n",
    "\n",
    "        x = conv_in(x)\n",
    "        for block in blocks:\n",
    "            x = block(x, t)\n",
    "        x = norm(x)\n",
    "        x = conv_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e037b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training.train_state import TrainState\n",
    "from models import EMA\n",
    "\n",
    "\n",
    "class State(TrainState):\n",
    "    ema: EMA\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, *, apply_fn, params, tx, ema: EMA, **kwargs):\n",
    "        return super().create(\n",
    "            apply_fn=apply_fn, params=params, tx=tx, ema=ema.init(params), **kwargs\n",
    "        )\n",
    "\n",
    "    def apply_gradients(self, *, grads, **kwargs):\n",
    "        return super().apply_gradients(grads=grads, **kwargs)\n",
    "        # self = super().apply_gradients(grads=grads, **kwargs)\n",
    "        # ema = self.ema.update(self.params)\n",
    "        # return self.replace(params=ema.params, ema=ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67120457",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "from jax_metrics.metrics import Mean, Metrics\n",
    "from models import UNet\n",
    "\n",
    "# module = SimpleUNet(units=64, emb_dim=32)\n",
    "# module = TimeConditionedMLP(2048)\n",
    "# module = SimpleCNN(64)\n",
    "# module = Mixer1D(\n",
    "#     patch_size=4,\n",
    "#     hidden_size=64,\n",
    "#     mix_patch_size=512,\n",
    "#     mix_hidden_size=512,\n",
    "#     num_blocks=4,\n",
    "#     num_steps=config.num_steps,\n",
    "# )\n",
    "# module = Mixer2D(64, 128, (1, 1), 4)\n",
    "module = UNet(dim=32, dim_mults=(1, 2, 4), channels=1)\n",
    "variables = module.init(jax.random.PRNGKey(42), X[:1], jnp.array([0]))\n",
    "# tx = optax.adamw(\n",
    "#     optax.linear_onecycle_schedule(\n",
    "#         config.total_samples // config.batch_size,\n",
    "#         2 * config.lr,\n",
    "#     )\n",
    "# )\n",
    "# tx = optax.adamw(optax.linear_schedule(config.lr, config.lr / 10, config.num_steps))\n",
    "tx = optax.adamw(config.lr)\n",
    "state = State.create(\n",
    "    apply_fn=module.apply, params=variables[\"params\"], tx=tx, ema=EMA(mu=0.6)\n",
    ")\n",
    "metrics = Metrics(Mean(name=\"loss\").map_arg(loss=\"values\")).init()\n",
    "\n",
    "print(module.tabulate(jax.random.PRNGKey(42), X[:1], jnp.array([0]), depth=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_diffusion(process, key, x, noise_hat, t):\n",
    "    betas = expand_to(process.betas[t], x)\n",
    "    alphas = expand_to(process.alphas[t], x)\n",
    "    alpha_bars = expand_to(process.alpha_bars[t], x)\n",
    "\n",
    "    sampling_noise = jnp.sqrt(betas) * jax.random.normal(key, x.shape)\n",
    "    noise_hat = betas / jnp.sqrt(1.0 - alpha_bars) * noise_hat\n",
    "    x = (x - noise_hat) / jnp.sqrt(alphas)\n",
    "\n",
    "    return jnp.where(expand_to(t, x) == 0, x, x + sampling_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, xt, t, noise):\n",
    "    noise_hat = state.apply_fn({\"params\": params}, xt, t)\n",
    "    return jnp.mean((noise - noise_hat) ** 2)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(key, x, state, metrics, process):\n",
    "    key_t, key_diffusion, key = jax.random.split(key, 3)\n",
    "    t = jax.random.uniform(\n",
    "        key_t, (x.shape[0],), minval=0, maxval=config.num_steps\n",
    "    ).astype(jnp.int32)\n",
    "    xt, noise = forward_diffusion(process, key_diffusion, x, t)\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params, xt, t, noise)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = metrics.update(loss=loss)\n",
    "    logs = metrics.compute()\n",
    "    return logs, key, state, metrics\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def sample(key, x0, ts, state, process):\n",
    "    keys = jax.random.split(key, len(ts))\n",
    "\n",
    "    def scan_fn(x, inputs):\n",
    "        t, key = inputs\n",
    "        t = jnp.full((x.shape[0],), t)\n",
    "        noise_hat = state.apply_fn({\"params\": state.params}, x, t)\n",
    "        x = reverse_diffusion(process, key, x, noise_hat, t)\n",
    "        return x, x\n",
    "\n",
    "    _, xs = jax.lax.scan(scan_fn, x0, (ts, keys))\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2e0d1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pkbar import Kbar\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "axs_diffusion = None\n",
    "axs_samples = None\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    kbar = Kbar(\n",
    "        target=config.steps_per_epoch,\n",
    "        epoch=epoch,\n",
    "        num_epochs=config.epochs,\n",
    "        width=16,\n",
    "        always_stateful=True,\n",
    "    )\n",
    "    metrics = metrics.reset()\n",
    "\n",
    "    for step in range(config.steps_per_epoch):\n",
    "        x = X[np.random.choice(np.arange(len(X)), config.batch_size)]\n",
    "        logs, key, state, metrics = train_step(key, x, state, metrics, process)\n",
    "        kbar.update(step, values=list(logs.items()))\n",
    "\n",
    "    # --------------------\n",
    "    # visualize progress\n",
    "    # --------------------\n",
    "    n_rows = 3\n",
    "    viz_key = jax.random.PRNGKey(0)\n",
    "    x = jax.random.uniform(viz_key, (n_rows, *X.shape[1:]), minval=-1, maxval=1)\n",
    "\n",
    "    ts = jnp.arange(config.num_steps)[::-1]\n",
    "    xs = sample(viz_key, x, ts, state, process)\n",
    "    xs = np.concatenate([x[None], xs], axis=0)\n",
    "\n",
    "    if axs_diffusion is None or get_ipython():\n",
    "        _, axs_diffusion = plt.subplots(n_rows, 5, figsize=(15, 3 * n_rows))\n",
    "    for r, ax_row in enumerate(axs_diffusion):\n",
    "        for i, ti in enumerate(jnp.linspace(0, len(xs) - 1, 5).astype(int)):\n",
    "            ax_row[i].clear()\n",
    "            render_image(xs[ti, r], ax_row[i])\n",
    "            ax_row[i].axis(\"off\")\n",
    "    show_interactive()\n",
    "    print()  # newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93002502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from base64 import b64encode\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from matplotlib import animation\n",
    "\n",
    "\n",
    "def plot_trajectory_2d(\n",
    "    xs: np.ndarray,\n",
    "    interval: int = 10,\n",
    "    repeat_delay: int = 1000,\n",
    "    step_size: int = 1,\n",
    "    end_pad: int = 500,\n",
    "):\n",
    "\n",
    "    xs = xs[::step_size]\n",
    "\n",
    "    # replace last sample to create a 'pause' effect\n",
    "    pad_end = einop(xs[-1], \"... -> batch ...\", batch=end_pad)\n",
    "    xs = np.concatenate([xs, pad_end], axis=0)\n",
    "\n",
    "    N = len(xs)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    scatter = plt.scatter(xs[0][:, 0], xs[0][:, 1], s=1)\n",
    "\n",
    "    def animate(i):\n",
    "        scatter.set_offsets(xs[i])\n",
    "        return [scatter]\n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig,\n",
    "        animate,\n",
    "        init_func=lambda: animate(0),\n",
    "        frames=np.linspace(0, N - 1, N, dtype=int),\n",
    "        interval=interval,\n",
    "        repeat_delay=repeat_delay,\n",
    "        blit=True,\n",
    "    )\n",
    "\n",
    "    if get_ipython():\n",
    "        with TemporaryDirectory() as tmpdir:\n",
    "            img_name = Path(tmpdir) / f\"diffusion.gif\"\n",
    "            anim.save(str(img_name), writer=\"pillow\", fps=60)\n",
    "            image_bytes = b64encode(img_name.read_bytes()).decode(\"utf-8\")\n",
    "\n",
    "        display(HTML(f\"\"\"<img src='data:image/gif;base64,{image_bytes}'>\"\"\"))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa68c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = jax.random.uniform(key, (1000, 2), minval=-1, maxval=1)\n",
    "ts = jnp.arange(config.num_steps)[::-1]\n",
    "xs = sample(key, x, ts, state, process)\n",
    "\n",
    "if get_ipython():\n",
    "    anim = plot_trajectory_2d(xs, step_size=2)\n",
    "else:\n",
    "    anim = plot_trajectory_2d(\n",
    "        xs, step_size=2, interval=100, repeat_delay=1000, end_pad=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Optional, Tuple\n",
    "\n",
    "\n",
    "def plot_density(model_fn: Callable[[Any, Any], Any], ts):\n",
    "    x = jnp.linspace(-1, 1, 100)\n",
    "    y = jnp.linspace(-1, 1, 100)\n",
    "    xx, yy = jnp.meshgrid(x, y)\n",
    "    X = jnp.stack([xx, yy], axis=-1)\n",
    "\n",
    "    def mass_fn(x, t):\n",
    "        t_ = jnp.full((1,), t)\n",
    "        x_ = x[None]\n",
    "        noise_hat = model_fn(x_, t_)\n",
    "        magnitud = jnp.linalg.norm(noise_hat, axis=-1, keepdims=False)\n",
    "        mass = jnp.exp(-magnitud)\n",
    "        return mass[0]\n",
    "\n",
    "    mass_fn = jax.jit(\n",
    "        jax.vmap(\n",
    "            jax.vmap(jax.vmap(mass_fn, in_axes=(0, None)), in_axes=(0, None)),\n",
    "            in_axes=(None, 0),\n",
    "            out_axes=-1,\n",
    "        )\n",
    "    )\n",
    "    mass = mass_fn(X, ts).mean(axis=-1)\n",
    "    plt.contourf(xx, yy, mass, levels=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723da11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plot_density(\n",
    "    model_fn=lambda x, t: state.apply_fn({\"params\": state.params}, x, t),\n",
    "    ts=jnp.array([0, 10, 20]),\n",
    ")\n",
    "show_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
